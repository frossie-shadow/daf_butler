===================================
Overview - What is the Data Butler?
===================================

The Data Butler was initially conceived as a manager for repositories or
collections of datasets that would be accessible using groups of
scientifically-meaningful key/value pairs.  Those datasets could be retrieved
as in-memory objects and persisted from in-memory objects.  The Butler would be
the generic I/O API that isolates scientific application code from the
underlying implementation in terms of storage formats, physical locations, data
staging, database mapping, etc.

This general concept turns out to be useful in other contexts as well, making
the Data Butler potentially able to serve as a data *router* (or hub or
switch).  Data can be published and sent to multiple locations based on the
Butler configuration.  Those locations may include "persistent storages" that
are actually dynamic (like displays or subscription streams) rather than
truly persistent storage.

===========
Definitions
===========

Repository
==========

A repository is a collection of datasets with an associated configuration.  It
may also contain metadata databases that assist with finding datasets and
maintaining their provenance.  A repository can be versioned in a
user-selectable manner.  Repositories can point to other repositories,
providing what is in effect a search path for datasets.  This allows
repositories to share access to datasets without copying data.

Dataset
=======

A dataset is the persisted form of an in-memory object.  It could be a single
item, a composite, or a collection.  Examples: `int`/`long`, `PropertySet`,
`ExposureF`, WCS, PSF, `set`/`list`/`dict`.

Dataset type
============

A label given to a group of datasets reflecting their meaning or usage (not
their persisted representation).  Each dataset type corresponds to exactly one
Python type.  Dataset types are used by convention by Tasks for their inputs
and outputs.  Examples: `calexp`, `src`, `icSrc`.

Dataset genre
=============

A labeled set of basic access characteristics serving as the basis for a group
of dataset types, used to define new dataset types.  The characteristics may
include code, template strings, and other configuration data.  Dataset genres
are often (but not necessarily) common to all dataset types with the same
Python type, making it easy for an application to select which genre is
applicable to a new dataset type that it is creating.

Storage
=======

A mechanism for reading/writing a dataset to/from an in-memory object.
Particular storage methods or classes are used for writing to and reading from
FITS files, databases, or other physical representations.

dataId
======

A dictionary of key/value pairs.  These start out as scientifically meaningful,
and then translations and additions are applied to them to provide more
concrete information that is eventually used to substitute into a storage
location template.

DataRef
=======

A `DataId` packaged with a `Butler` for access to datasets.  A `DataRef` an be
used with multiple dataset types (if the keys are appropriate).

DataRefSet
==========

Logically, a set of 'DataRef's.  This may be implemented as an
iterator/generator in some contexts where materializing the set would be
expensive.  The 'DataRefSet' is usually generated by listing existing datasets
of a particular dataset type, but its component 'DataRef's can be used with
other dataset types.

Butler
======

The overall interface and manager for repositories.  The Butler takes zero or
more read-only input repositories and manages a single read/write output
repository.  The input repositories are recorded in the output repository.  The
initial output repository configuration is derived from the input repository
configurations along with user overrides.  Application-level tasks can add to
the output repository configuration, in particular by defining new dataset
types.

Input repositories may represent, among other things, science data, calibration
data to be used with the science data, or metadata.  The Butler provides
mechanisms to search through the set of repositories for a given dataset based
on its dataset type and data identifier (`DataId`).  
